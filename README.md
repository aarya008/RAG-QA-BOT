# ğŸ§  RAGChat Bot â€” PDF Question Answering using LangChain, Gemini & Gradio

### ğŸš€ An intelligent chatbot that reads your documents, understands your questions, and gives precise answers â€” powered by **Retrieval-Augmented Generation (RAG)** with **LangChain**, **Google Gemini**, and **HuggingFace Embeddings**.

---

## ğŸ—‚ï¸ Project Overview

This project implements a **RAG-based chatbot** capable of answering user queries from uploaded **PDF documents**.
It combines **Large Language Models (LLMs)** with a **Vector Database (Chroma)** to retrieve and reason over document content, making it an efficient and scalable **Document Q&A System**.

**Tech Stack Used:**

* ğŸ§© **LangChain** â€” RAG pipeline & data orchestration
* ğŸª¶ **Google Gemini (Generative AI)** â€” LLM for reasoning and answering
* ğŸ¤— **HuggingFace Sentence Transformers** â€” for high-quality text embeddings
* ğŸ§± **ChromaDB** â€” vector store for semantic search and retrieval
* âš™ï¸ **Gradio** â€” frontend UI for chatbot interaction
* ğŸ **Python** (with PyTorch) â€” core programming and model integration

---

## âœ¨ Features

âœ… Upload any **PDF file** up to 100 MB
âœ… Automatically extracts and splits text into chunks
âœ… Generates **semantic embeddings** using HuggingFace
âœ… Stores document knowledge in a **persistent vector database**
âœ… Retrieves contextually relevant information using **Chroma retriever**
âœ… Provides accurate and **LLM-powered answers** using Google Gemini
âœ… Clean and modern **Gradio chatbot interface**
âœ… Built with scalability and GPU support (CUDA/MPS detection)

---

## ğŸ§© Project Architecture

ğŸ“„ PDF Upload
â¬‡ï¸
ğŸ“š Document Loader (LangChain)
â¬‡ï¸
âœ‚ï¸ Text Splitter
â¬‡ï¸
ğŸ”¢ HuggingFace Embeddings
â¬‡ï¸
ğŸ’¾ Chroma Vector Database (Persistent Storage)
â¬‡ï¸
ğŸ” Retriever
â¬‡ï¸
ğŸ§  Google Gemini LLM
â¬‡ï¸
ğŸ’¬ Gradio Frontend Chatbot
â¬‡ï¸
ğŸ‘¤ User Interaction


## ğŸ’¡ Core Functionalities

| Function                | Description                                      |
| ----------------------- | ------------------------------------------------ |
| `upload_file()`         | Handles PDF uploads and vector DB persistence    |
| `vector_database()`     | Builds or loads existing embeddings for a file   |
| `retriever_qa()`        | Core RAG logic for contextual question-answering |
| `get_llm()`             | Initializes Google Gemini (Gemini 2.5 Flash)     |
| `get_embedding_model()` | Uses sentence-transformers/all-mpnet-base-v2     |
| `gradio_qa()`           | Connects Gradio frontend to backend logic        |

---

## ğŸ§° Installation & Setup

### 1ï¸âƒ£ Clone this repository

```bash
git clone https://github.com/<your-username>/RAGChatBot.git
cd RAGChatBot
```

### 2ï¸âƒ£ Create a virtual environment

```bash
python -m venv venv
source venv/bin/activate      # (Linux/Mac)
venv\Scripts\activate         # (Windows)
```

### 3ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Add your API key

Edit `ragbot.py` and replace:

```python
GOOGLE_API_KEY = "YOUR_API_KEY"
```

or export it as an environment variable:

```bash
export GOOGLE_API_KEY="your_google_api_key"
```

### 5ï¸âƒ£ Run the chatbot

```bash
python ragbot.py
```

Open the link in your terminal (`http://127.0.0.1:7860`) to chat with your bot.

---

## ğŸ–¼ï¸ Demo Preview

*(Insert screenshots or GIFs of your chatbot interface here)*

---

## ğŸ§  Skills Demonstrated

* Retrieval-Augmented Generation (RAG)
* LangChain workflow engineering
* LLM integration (Google Gemini)
* Vector Databases (Chroma)
* Natural Language Processing (NLP)
* PDF text extraction and document parsing
* UI development with Gradio
* GPU-aware model optimization (PyTorch)

---

## ğŸ§¾ Example Usage

1. Upload your PDF (research paper, report, documentation, etc.)
2. Ask: *â€œSummarize chapter 2 in simple terms.â€*
3. The chatbot retrieves relevant content and gives a **concise, contextual, and AI-generated answer**.

---

## âš–ï¸ License

This project is licensed under the **MIT License** â€” youâ€™re free to use and modify it with attribution.
(You can switch to Apache-2.0 if you want stronger protection for derivative works.)

---

## ğŸ‘¨â€ğŸ’» Author

**Aarya Tagare**
ğŸ“ Electrical Engineer | ğŸ’¡ Passionate about AI, ML, and Generative AI | Exploring Agentic AI Systems & Intelligent Automation 
ğŸ“ Kolhapur, India
ğŸ”— [LinkedIn](www.linkedin.com/in/aary-tagare14) â€¢ [GitHub](https://github.com/aarya008) â€¢ [Email](mailto:tagareaary@gmail.com)

---

